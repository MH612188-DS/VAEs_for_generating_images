
# Variational Autoencoder (VAE) on MNIST

This project implements a **Variational Autoencoder (VAE)** using TensorFlow and Keras to learn a 2D latent representation of the MNIST handwritten digits dataset. The model is trained to reconstruct input images while learning a meaningful latent space for generative sampling.

---

## ğŸ” Overview

- **Dataset**: MNIST (handwritten digits, 28x28 grayscale images)
- **Latent Dimension**: 2 (for visualization)
- **Architecture**:
  - Convolutional Encoder
  - Latent Sampling with Reparameterization Trick
  - Convolutional Decoder (using Conv2DTranspose)
- **Loss Function**:
  - Binary Cross-Entropy (Reconstruction Loss)
  - KL Divergence (Latent Regularization)

---

## ğŸ“‚ Project Structure

```bash
.
â”œâ”€â”€ main.py                # Main script for training and evaluation
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ vae.keras          # Saved VAE model
â”‚   â”œâ”€â”€ encoder.keras      # Saved encoder
â”‚   â””â”€â”€ decoder.keras      # Saved decoder
â”œâ”€â”€ checkpoint.keras       # Best model checkpoint
â”œâ”€â”€ logs/                  # TensorBoard logs
â”œâ”€â”€ README.md              # Project documentation
